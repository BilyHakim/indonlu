{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtbRn899TOug"
      },
      "source": [
        "# Tutorial NLU - Finetuning SmSA\n",
        "SmSA is a Sentiment Analysis dataset with 3 possible labels: `positive`, `negative`, and `neutral`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WIfvxWJ9TOuk",
        "outputId": "cc2a5980-2940-47a4-a748-94ae7f29aa50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-788d794e091a>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os, sys\n",
        "sys.path.append('../')\n",
        "os.chdir('../')\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import fasttext\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from utils.forward_fn import forward_sequence_classification\n",
        "from utils.metrics import document_sentiment_metrics_fn\n",
        "from utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q7WLvUiTOum"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# common functions\n",
        "###\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "def count_param(module, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "    \n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def metrics_to_string(metric_dict):\n",
        "    string_list = []\n",
        "    for key, value in metric_dict.items():\n",
        "        string_list.append('{}:{:.2f}'.format(key, value))\n",
        "    return ' '.join(string_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYxyhTOeTOun"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "set_seed(26092020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrgnqgdjTOun"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UGILb0DTOuo"
      },
      "outputs": [],
      "source": [
        "train_dataset_path = './dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
        "valid_dataset_path = './dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
        "test_dataset_path = './dataset/smsa_doc-sentiment-prosa/test_preprocess.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MldAcEGrTOuo"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_dataset_path, sep='\\t', header=None)\n",
        "valid_df = pd.read_csv(valid_dataset_path, sep='\\t', header=None)\n",
        "test_df = pd.read_csv(test_dataset_path, sep='\\t', header=None)\n",
        "\n",
        "train_df.columns = ['text', 'label']\n",
        "valid_df.columns = ['text', 'label']\n",
        "test_df.columns = ['text', 'label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0JaE--jTOuo",
        "outputId": "cdecb826-21cf-4235-d4e0-4afc020794a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>makanan beragam , harga makanan di food stall ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>pakai kartu kredit bca tidak untung malah rugi...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tempat unik , bagus buat foto , makanan enak ,...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>saya bersama keluarga baru saja menikmati peng...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bersyukur</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text     label\n",
              "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
              "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
              "2  lokasi strategis di jalan sumatera bandung . t...  positive\n",
              "3  betapa bahagia nya diri ini saat unboxing pake...  positive\n",
              "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
              "5  makanan beragam , harga makanan di food stall ...  positive\n",
              "6  pakai kartu kredit bca tidak untung malah rugi...  negative\n",
              "7  tempat unik , bagus buat foto , makanan enak ,...  positive\n",
              "8  saya bersama keluarga baru saja menikmati peng...  positive\n",
              "9                                          bersyukur  positive"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUvCSflvTOuq"
      },
      "outputs": [],
      "source": [
        "def get_label_idx(label):\n",
        "    if label == 'positive':\n",
        "        return 2\n",
        "    if label == 'negative':\n",
        "        return 1\n",
        "    if label == 'neutral':\n",
        "        return 0\n",
        "train_df['label'] = train_df['label'].apply(get_label_idx)\n",
        "valid_df['label'] = valid_df['label'].apply(get_label_idx)\n",
        "test_df['label'] = test_df['label'].apply(get_label_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZMsefmsTOuq",
        "outputId": "75d09ab3-daf5-4721-fb37-6f18abea8cab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>makanan beragam , harga makanan di food stall ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>pakai kartu kredit bca tidak untung malah rugi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tempat unik , bagus buat foto , makanan enak ,...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>saya bersama keluarga baru saja menikmati peng...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bersyukur</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  warung ini dimiliki oleh pengusaha pabrik tahu...      2\n",
              "1  mohon ulama lurus dan k212 mmbri hujjah partai...      0\n",
              "2  lokasi strategis di jalan sumatera bandung . t...      2\n",
              "3  betapa bahagia nya diri ini saat unboxing pake...      2\n",
              "4  duh . jadi mahasiswa jangan sombong dong . kas...      1\n",
              "5  makanan beragam , harga makanan di food stall ...      2\n",
              "6  pakai kartu kredit bca tidak untung malah rugi...      1\n",
              "7  tempat unik , bagus buat foto , makanan enak ,...      2\n",
              "8  saya bersama keluarga baru saja menikmati peng...      2\n",
              "9                                          bersyukur      2"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBpKn8G2TOur"
      },
      "source": [
        "# Traditional Approach\n",
        "- Bag of Word\n",
        "- TF-IDF\n",
        "- Word Vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-lHb0PaTOur"
      },
      "source": [
        "# Bag-of-Word Model\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fixb63y5TOur"
      },
      "source": [
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/bag_of_word.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgAWOKsKTOus"
      },
      "source": [
        "## Count Vectorizer (default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-v6MaGkTOus"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "train_input = vectorizer.fit_transform(train_df['text'])\n",
        "valid_input = vectorizer.transform(valid_df['text'])\n",
        "test_input = vectorizer.transform(test_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp3wfuTRTOus",
        "outputId": "5880d679-c931-4d5d-970c-fa807cea5a4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17241,\n",
              " array(['00', '000', '001', '01', '010', '0111', '011770465655617', '02',\n",
              "        '021', '022', '030360019614718', '0361', '04', '05', '0561', '07',\n",
              "        '08', '081147286649', '081377744845', '08156189559'], dtype=object))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI9imRgQTOut",
        "outputId": "46e80e4e-73c7-488d-f965-1baf901c8124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 47.7 s, sys: 1min 37s, total: 2min 25s\n",
            "Wall time: 5.16 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samuel/anaconda2/envs/env_nusa_exp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9BhFp-8TOut",
        "outputId": "736e6ab2-427a-4acd-be12-83c04634c97c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 566 ms, sys: 1.15 s, total: 1.72 s\n",
            "Wall time: 57.1 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.9810909090909091,\n",
              " 'F1': 0.9829617694342899,\n",
              " 'REC': 0.9828632926562729,\n",
              " 'PRE': 0.9831031020207582}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYGtiBMQTOut",
        "outputId": "2039d199-304b-4183-f112-ae6ad4bdc85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 50.6 ms, sys: 41.7 ms, total: 92.4 ms\n",
            "Wall time: 8.99 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8746031746031746,\n",
              " 'F1': 0.8383965071776482,\n",
              " 'REC': 0.8267771483892248,\n",
              " 'PRE': 0.8546862276279086}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0c7zzQzTOuu",
        "outputId": "eaea3fee-c2e4-4cc3-8b07-ea5cc4f7d257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5.68 ms, sys: 124 µs, total: 5.81 ms\n",
            "Wall time: 5.02 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.782,\n",
              " 'F1': 0.7395238527221748,\n",
              " 'REC': 0.7219508432743726,\n",
              " 'PRE': 0.7948786330245928}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sirTRNtNTOuu"
      },
      "source": [
        "## Count Vectorizer (N-Gram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5JPThtnTOuv"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "train_input = vectorizer.fit_transform(train_df['text'])\n",
        "valid_input = vectorizer.transform(valid_df['text'])\n",
        "test_input = vectorizer.transform(test_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B19i1kjWTOuv",
        "outputId": "f82eed92-416a-4352-99ad-10eda33872e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(391358,\n",
              " array(['00', '00 04', '00 04 00', '00 16', '00 16 00', '00 21',\n",
              "        '00 21 30', '00 agak', '00 agak mahal', '00 agar', '00 agar tidak',\n",
              "        '00 atau', '00 atau sampai', '00 dan', '00 dan dari',\n",
              "        '00 dan masih', '00 dan setiap', '00 dari', '00 dari menu',\n",
              "        '00 disodori'], dtype=object))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DxHFtujTOuv",
        "outputId": "0283bed9-5924-4e55-c820-4572a8fc0878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 19s, sys: 9min 4s, total: 15min 24s\n",
            "Wall time: 45.2 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samuel/anaconda2/envs/env_nusa_exp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbv92SAVTOuw",
        "outputId": "5e5d75d3-e755-4bd6-bb3d-19223aeee442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 35.3 ms, sys: 0 ns, total: 35.3 ms\n",
            "Wall time: 33 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.9983636363636363,\n",
              " 'F1': 0.9985637461674889,\n",
              " 'REC': 0.9989747207456717,\n",
              " 'PRE': 0.9981560629716001}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG6eiDcoTOuw",
        "outputId": "31b67961-d1e9-4ebc-e7b6-3d906bd2b9fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 9.59 ms, sys: 0 ns, total: 9.59 ms\n",
            "Wall time: 8.23 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8904761904761904,\n",
              " 'F1': 0.8475910022128509,\n",
              " 'REC': 0.8310159656272837,\n",
              " 'PRE': 0.8721277704167449}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p-l9k3yTOuw",
        "outputId": "d2bad22d-d7b6-4a92-a5ef-3e49ffba7b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8.44 ms, sys: 0 ns, total: 8.44 ms\n",
            "Wall time: 7 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.784,\n",
              " 'F1': 0.7315185957006912,\n",
              " 'REC': 0.7173116915763975,\n",
              " 'PRE': 0.7889916271300157}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp1sw0KsTOuw"
      },
      "source": [
        "## Count Vectorizer (N-Gram + Filtering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gN0N0GHTOuw"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(min_df=3, ngram_range=(1,3))\n",
        "train_input = vectorizer.fit_transform(train_df['text'])\n",
        "valid_input = vectorizer.transform(valid_df['text'])\n",
        "test_input = vectorizer.transform(test_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMH5gyM5TOux",
        "outputId": "8d10ae0b-e5e8-471f-ad44-95f651df8396"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31829,\n",
              " array(['00', '00 dan', '000', '000 dan', '000 orang', '000 per',\n",
              "        '000 per porsi', '000 porsi', '000 rp', '000 rupiah', '000 untuk',\n",
              "        '01', '021', '05', '07', '07 00', '08', '09', '09 00', '10'],\n",
              "       dtype=object))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt2a73cETOux",
        "outputId": "03b3c63c-55fd-48f5-f0a2-d49c47fd52d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 4s, sys: 2min 9s, total: 3min 14s\n",
            "Wall time: 6.57 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samuel/anaconda2/envs/env_nusa_exp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-rLSQlfTOux",
        "outputId": "057f8d1a-856f-4404-bfca-8e75e0f46276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 610 ms, sys: 1.3 s, total: 1.91 s\n",
            "Wall time: 63 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.9895454545454545,\n",
              " 'F1': 0.9904695485530183,\n",
              " 'REC': 0.991719969502986,\n",
              " 'PRE': 0.9892579704727803}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-5xRkpCTOux",
        "outputId": "bc54c43b-3a6c-4838-d422-a78c75e8af89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4.54 ms, sys: 722 µs, total: 5.26 ms\n",
            "Wall time: 4.97 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8865079365079365,\n",
              " 'F1': 0.8452497775910217,\n",
              " 'REC': 0.8406379309451012,\n",
              " 'PRE': 0.8509939811942893}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWjBao5-TOux",
        "outputId": "e1c87e9f-7a30-4456-ee8a-e77a234f70ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6.7 ms, sys: 0 ns, total: 6.7 ms\n",
            "Wall time: 5.73 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.818,\n",
              " 'F1': 0.7789645345628964,\n",
              " 'REC': 0.7641916906622788,\n",
              " 'PRE': 0.8172109111408878}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk9ArIJWTOuy"
      },
      "source": [
        "## Count Vectorizer (N-Gram + Filtering v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhh7b0rFTOuy"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(min_df=5, ngram_range=(1,4), max_features=10000)\n",
        "train_input = vectorizer.fit_transform(train_df['text'])\n",
        "valid_input = vectorizer.transform(valid_df['text'])\n",
        "test_input = vectorizer.transform(test_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDxgOAioTOuy",
        "outputId": "1731fbf1-b388-4c92-cfe7-1b61d11d0d1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000,\n",
              " array(['00', '000', '000 untuk', '10', '10 ribu', '10 tahun', '100',\n",
              "        '100 000', '100 ribu', '1000', '11', '12', '13', '14', '15',\n",
              "        '15 menit', '15 ribu', '150', '16', '17'], dtype=object))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Pbl2tCTOuy",
        "outputId": "0bc7f04f-8aa6-4510-a3ae-2c10103caa44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 54.4 s, sys: 1min 52s, total: 2min 46s\n",
            "Wall time: 5.54 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samuel/anaconda2/envs/env_nusa_exp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2RPCuflTOuy",
        "outputId": "9526dbe8-0e83-46e7-e4d7-069a072bdeb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 276 ms, sys: 672 ms, total: 948 ms\n",
            "Wall time: 31.5 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.9815454545454545,\n",
              " 'F1': 0.9773292783607032,\n",
              " 'REC': 0.9821751510041,\n",
              " 'PRE': 0.9726701439372767}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjobjbgCTOuy",
        "outputId": "405ab516-e7e4-41e3-8699-62486e01acb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 71.9 ms, sys: 216 ms, total: 288 ms\n",
            "Wall time: 9.74 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8888888888888888,\n",
              " 'F1': 0.8432603991514438,\n",
              " 'REC': 0.8443535288593939,\n",
              " 'PRE': 0.8430157748514255}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iz-SOxhTOuz",
        "outputId": "22658ee7-0075-4973-a618-dd631b7366d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 90.9 ms, sys: 122 ms, total: 212 ms\n",
            "Wall time: 7.2 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.824,\n",
              " 'F1': 0.7810418941195736,\n",
              " 'REC': 0.7646287535993418,\n",
              " 'PRE': 0.8269000798884538}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYxsrIlNTOuz"
      },
      "source": [
        "## Count Vectorizer (300D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCrAYDpWTOuz"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(1,3), max_features=300)\n",
        "train_input = vectorizer.fit_transform(train_df['text'])\n",
        "valid_input = vectorizer.transform(valid_df['text'])\n",
        "test_input = vectorizer.transform(test_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSG3eqTJTOuz",
        "outputId": "06607041-b3fb-4bf3-a3a8-95c32ed14c20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300,\n",
              " array(['ada', 'ada di', 'ada yang', 'adalah', 'agak', 'akan', 'aku',\n",
              "        'anak', 'anda', 'apa', 'apalagi', 'area', 'atas', 'atau', 'ayam',\n",
              "        'bagi', 'bagus', 'baik', 'bakar', 'bakso'], dtype=object))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xQ3QADkTOuz",
        "outputId": "619ce362-a052-42f1-ad2d-8ea4bf09538c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 366 ms, sys: 1.53 ms, total: 367 ms\n",
            "Wall time: 367 ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samuel/anaconda2/envs/env_nusa_exp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tJvQPo3TOu0",
        "outputId": "558dc070-69b2-4ecf-f2bf-7cd003744116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 14.7 ms, sys: 1.81 ms, total: 16.5 ms\n",
            "Wall time: 14.3 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8342727272727273,\n",
              " 'F1': 0.7842994615016484,\n",
              " 'REC': 0.8017245144441164,\n",
              " 'PRE': 0.7706329981964172}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggel5jfvTOu0",
        "outputId": "58e3d282-c195-4ef5-fd9f-6f631945be0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6.86 ms, sys: 0 ns, total: 6.86 ms\n",
            "Wall time: 5.79 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8063492063492064,\n",
              " 'F1': 0.7398947719823138,\n",
              " 'REC': 0.7552179368796123,\n",
              " 'PRE': 0.7286074612213174}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyCrS-vvTOu0",
        "outputId": "0a5a4c90-9458-4608-8525-b6080b2b1513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5.3 ms, sys: 1.06 ms, total: 6.36 ms\n",
            "Wall time: 5.17 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.656,\n",
              " 'F1': 0.6007126805728679,\n",
              " 'REC': 0.5946084144613556,\n",
              " 'PRE': 0.6588807430212555}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "WsggmO-pTOu0"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noolP8HRTOu0"
      },
      "source": [
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/bag_of_word.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIlb1v6uTOu0"
      },
      "source": [
        "DF(about) = 2 &nbsp; | &nbsp; DF(bird) = 3 &nbsp; | &nbsp; DF(heard) = 1  &nbsp; | &nbsp; DF(is) = 1 &nbsp; | &nbsp; DF(the) = 3 &nbsp; | &nbsp; DF(word) = 1 &nbsp; | &nbsp; DF(you) = 1<br/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow5X5jibTOu1"
      },
      "source": [
        "## Formula\n",
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/tf_idf.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cw3WkqITOu1"
      },
      "source": [
        "## TF-IDF Vectorizer\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t852eObnTOu1"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "train_input = vectorizer.fit_transform(train_df['text'])\n",
        "valid_input = vectorizer.transform(valid_df['text'])\n",
        "test_input = vectorizer.transform(test_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3CG53i-TOu1",
        "outputId": "db75ba3e-e709-43e6-cee9-eca4ec225c1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17241,\n",
              " array(['00', '000', '001', '01', '010', '0111', '011770465655617', '02',\n",
              "        '021', '022', '030360019614718', '0361', '04', '05', '0561', '07',\n",
              "        '08', '081147286649', '081377744845', '08156189559'], dtype=object))"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j1UMColTOu1",
        "outputId": "36b317ec-7fb1-420b-bd1d-f99186fa790b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 43.6 s, sys: 1min 25s, total: 2min 9s\n",
            "Wall time: 4.3 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samuel/anaconda2/envs/env_nusa_exp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVzZU8ZITOu2",
        "outputId": "05f03f0a-2647-4640-dd68-3b78d9b79fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 537 ms, sys: 1.17 s, total: 1.71 s\n",
            "Wall time: 56.7 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.9352727272727273,\n",
              " 'F1': 0.9284235049152748,\n",
              " 'REC': 0.9162891631604279,\n",
              " 'PRE': 0.9423197196893929}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odMu0lsOTOu2",
        "outputId": "842a03ec-5918-429b-b21f-fa6d7d05458a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 133 ms, sys: 301 ms, total: 434 ms\n",
            "Wall time: 14.8 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8746031746031746,\n",
              " 'F1': 0.8351209923387012,\n",
              " 'REC': 0.8135745131288976,\n",
              " 'PRE': 0.8669546384817232}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV6f521zTOu2",
        "outputId": "e8d0b763-1617-4b82-be7a-7384bd84000c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4.95 ms, sys: 0 ns, total: 4.95 ms\n",
            "Wall time: 4.31 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.73,\n",
              " 'F1': 0.6679165117865385,\n",
              " 'REC': 0.6542175145116321,\n",
              " 'PRE': 0.7709099094573956}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dumdBQuKTOu2"
      },
      "source": [
        "## TF-IDF Vectorizer + Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MziAlfWSTOu2"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=3, ngram_range=(1,3), max_features=20000)\n",
        "train_input = vectorizer.fit_transform(train_df['text'])\n",
        "valid_input = vectorizer.transform(valid_df['text'])\n",
        "test_input = vectorizer.transform(test_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKNh-eZaTOu3",
        "outputId": "e52bdfd4-591d-42e8-ae2f-58455b8e38f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20000,\n",
              " array(['00', '000', '000 dan', '000 orang', '000 per', '000 per porsi',\n",
              "        '000 porsi', '000 rupiah', '000 untuk', '01', '05', '09', '10',\n",
              "        '10 000', '10 malam', '10 menit', '10 orang', '10 pagi', '10 ribu',\n",
              "        '10 tahun'], dtype=object))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSQ1KpjITOu3",
        "outputId": "1dfed0d1-84eb-4558-9eda-2929654ab838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 52.8 s, sys: 1min 42s, total: 2min 35s\n",
            "Wall time: 5.2 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samuel/anaconda2/envs/env_nusa_exp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgLKNTjCTOu3",
        "outputId": "e4ee4923-1bb6-45ba-9aaa-0300412b5a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 363 ms, sys: 874 ms, total: 1.24 s\n",
            "Wall time: 41.3 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.94,\n",
              " 'F1': 0.9273045632388275,\n",
              " 'REC': 0.9122309581630504,\n",
              " 'PRE': 0.9449576176074709}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba1ivLYFTOu3",
        "outputId": "35e00145-bcb0-4a1d-ddd8-3fedf668240e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 159 ms, sys: 336 ms, total: 495 ms\n",
            "Wall time: 16.4 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8817460317460317,\n",
              " 'F1': 0.8360318397305712,\n",
              " 'REC': 0.8109905932639521,\n",
              " 'PRE': 0.8742494254266702}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG2UcQ-hTOu3",
        "outputId": "cab02147-cc8f-4b49-98e3-a73cae21a739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 86.7 ms, sys: 70.4 ms, total: 157 ms\n",
            "Wall time: 6.82 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.76,\n",
              " 'F1': 0.6775857115182596,\n",
              " 'REC': 0.6697032542620778,\n",
              " 'PRE': 0.7881429681429681}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT9StHEcTOu3"
      },
      "source": [
        "## TF-IDF (300D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX9QN_m0TOu4"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=3, ngram_range=(1,3), max_features=300)\n",
        "train_input = vectorizer.fit_transform(train_df['text'])\n",
        "valid_input = vectorizer.transform(valid_df['text'])\n",
        "test_input = vectorizer.transform(test_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLaEXYeMTOu4",
        "outputId": "b8afe9f8-3bb2-4306-b5d6-0a5e4e16b136"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300,\n",
              " array(['ada', 'ada di', 'ada yang', 'adalah', 'agak', 'akan', 'aku',\n",
              "        'anak', 'anda', 'apa', 'apalagi', 'area', 'atas', 'atau', 'ayam',\n",
              "        'bagi', 'bagus', 'baik', 'bakar', 'bakso'], dtype=object))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out()), vectorizer.get_feature_names_out()[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHJww6HCTOu4",
        "outputId": "d6c1b429-4cd5-4966-900b-fc1832c51ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 370 ms, sys: 1.72 ms, total: 372 ms\n",
            "Wall time: 371 ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/samuel/anaconda2/envs/env_nusa_exp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaNaTFTeTOu4",
        "outputId": "2b393b7f-0904-428e-8605-fcf58ae00a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 14.8 ms, sys: 1.25 ms, total: 16 ms\n",
            "Wall time: 14.3 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8262727272727273,\n",
              " 'F1': 0.7680363475976594,\n",
              " 'REC': 0.7660091038195164,\n",
              " 'PRE': 0.7702586980596919}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNukOkf_TOu4",
        "outputId": "ee8a34bf-d9a5-4c8d-a133-76f795d1751d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3.13 ms, sys: 2.58 ms, total: 5.71 ms\n",
            "Wall time: 4.88 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.807936507936508,\n",
              " 'F1': 0.7297792963402681,\n",
              " 'REC': 0.7275002115388721,\n",
              " 'PRE': 0.7329049003305242}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he5HxymqTOu4",
        "outputId": "a27e336c-ee1f-4888-ed41-684da0faae0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4.3 ms, sys: 368 µs, total: 4.67 ms\n",
            "Wall time: 4.41 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.61,\n",
              " 'F1': 0.55401489537221,\n",
              " 'REC': 0.5486939531057179,\n",
              " 'PRE': 0.6182570483385178}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gu_rRjYTOu5"
      },
      "source": [
        "# Word Vector Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYgwZTi2TOu5"
      },
      "source": [
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/word_vector.png?raw=1\"/>\n",
        "\n",
        "**== Widely-used Word Vector Model ==**\n",
        "- CBOW (Continuous Bag-of-word)\n",
        "- Skip-Gram -> **FastText**\n",
        "- GLoVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc7e_pf2TOu5"
      },
      "source": [
        "Download pre-trained FastText model (support 157 languages): https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/fasttext.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W2A_RHgTOu5"
      },
      "outputs": [],
      "source": [
        "# Uncommnet and run the following line to download indonesian (id) fasttext embedding\n",
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz cc.id.300.bin.gz\n",
        "# !gunzip cc.id.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euh1Q6V7TOu5",
        "outputId": "c0106ff5-2443-4f77-84a4-b46c26bc1c69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "wv_model = fasttext.load_model(\"./tutorial/cc.id.300.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QafSUBprTOu6"
      },
      "outputs": [],
      "source": [
        "def encode_fasttext(sentence):\n",
        "    word_vectors = []\n",
        "    for word in word_tokenize(sentence):\n",
        "        word_vectors.append(wv_model[word])\n",
        "    return np.stack(word_vectors)\n",
        "\n",
        "train_df['vector'] = train_df['text'].apply(encode_fasttext)\n",
        "valid_df['vector'] = valid_df['text'].apply(encode_fasttext)\n",
        "test_df['vector'] = test_df['text'].apply(encode_fasttext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZs-Oj6ATOu6"
      },
      "outputs": [],
      "source": [
        "train_input = train_df['vector'].apply(lambda x: np.mean(x, axis=0)).tolist()\n",
        "valid_input = valid_df['vector'].apply(lambda x: np.mean(x, axis=0)).tolist()\n",
        "test_input = test_df['vector'].apply(lambda x: np.mean(x, axis=0)).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuABbJr0TOu6",
        "outputId": "4a746361-6ef6-4041-d0f6-f923f7dc747f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 23.3 s, sys: 20.6 s, total: 44 s\n",
            "Wall time: 1.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression()\n",
        "model = model.fit(train_input, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crvngJLyTOu6",
        "outputId": "0a48783e-5507-4cd7-bcfe-43106d18d914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.91 s, sys: 2.98 s, total: 4.89 s\n",
            "Wall time: 733 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8449090909090909,\n",
              " 'F1': 0.8083042451537518,\n",
              " 'REC': 0.782718918024996,\n",
              " 'PRE': 0.843391795515397}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(train_input)\n",
        "document_sentiment_metrics_fn(hyps, train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTUn9F--TOu7",
        "outputId": "86d9226d-1098-4a4e-b4b3-9bf1879f4d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 217 ms, sys: 264 ms, total: 481 ms\n",
            "Wall time: 16.4 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.8420634920634921,\n",
              " 'F1': 0.791869039515769,\n",
              " 'REC': 0.7703950140265868,\n",
              " 'PRE': 0.8206850094183347}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(valid_input)\n",
        "document_sentiment_metrics_fn(hyps, valid_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWiYLOy_TOu7",
        "outputId": "3bcb69e5-cd17-44fb-e16f-541a8c8c5c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 202 ms, sys: 247 ms, total: 449 ms\n",
            "Wall time: 15.3 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ACC': 0.67,\n",
              " 'F1': 0.6037776632851872,\n",
              " 'REC': 0.5947426756250285,\n",
              " 'PRE': 0.7024387416556741}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "hyps = model.predict(test_input)\n",
        "document_sentiment_metrics_fn(hyps, test_df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "GktKRHiaTOu7"
      },
      "source": [
        "# Deep Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xee6n1FbTOu7"
      },
      "source": [
        "## Background\n",
        "Previous approach only handle a fixed amount of features for predictions, requiring statistics derived features from the original sequence which causing loss of information from the original sequence. Can we make it better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "w_qjN4pZTOu7"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pmaSD7sTOu7"
      },
      "source": [
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/logistic_regression.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H49GPYbjTOu8"
      },
      "source": [
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/logistic_regression.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "X-AcpC9aTOu8"
      },
      "source": [
        "#### Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELgPP2F0TOu8"
      },
      "source": [
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/backprop.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PiuYz-HTOu8"
      },
      "source": [
        "#### Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-KBMcH2TOu8"
      },
      "source": [
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/gradient_descent.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqxZ9AGrTOu8"
      },
      "source": [
        "#### Chain Rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzoSdUreTOu8"
      },
      "source": [
        "<div style=\"background-color: white; padding:10px;\">\n",
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/chain_rule.png?raw=1\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba1mSJqkTOu9"
      },
      "source": [
        "## Types of Deep Learning / Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhlRsJlYTOu9"
      },
      "source": [
        "#### Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPw7pY67TOu9"
      },
      "source": [
        "<div style=\"background-color: white; text-align: center;\">\n",
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/mlp.png?raw=1\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "b7_N1vBQTOu9"
      },
      "source": [
        "#### Convolution Neural Network (CNN)\n",
        "- Convolutional Neural Networks for Visual Recognition (Stanford CS231) - https://cs231n.github.io/convolutional-networks/\n",
        "- CNN Tutorial (Udacity) - https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks\n",
        "- But what is a convolution? (3blue1brown) - https://www.youtube.com/watch?v=KuXjwB4LzSA\n",
        "\n",
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/cnn.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPwY1eNUTOu9"
      },
      "source": [
        "#### Recurrent Neural Network (RNN)\n",
        "- Cheatsheet RNN (Stanford CS230) - https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
        "- RNN Tutorial (Udacity) - https://github.com/udacity/deep-learning-v2-pytorch/tree/master/recurrent-neural-networks\n",
        "- Sentiment Analysis w/ RNN (Udacity) - https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn\n",
        "\n",
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/rnn.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BHutEqkTOu9"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP2HbFfaTOu9"
      },
      "source": [
        "#### Why Transformer?\n",
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/why_transformer.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLebKS5ITOu-"
      },
      "source": [
        "#### Variant of Transformer Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppLitF6qTOu-"
      },
      "source": [
        "- Encoder-Only: BERT, RoBERTa, ALBERT, ELECTRA, mBERT, XLM-R, ...\n",
        "- Decoder-Only: GPT2, GPT3, BLOOM, ...\n",
        "- Encoder-Decoder: BART, T5, mBART, mT5, T0, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rDVRXnbTOu-"
      },
      "source": [
        "#### Pre-trained Language Models\n",
        "- Transformer needs a huge amount of data to train\n",
        "- To mitigate this problem, many researchers have built various pre-trained transformer models (pretrained LM)\n",
        "- Rather than training the transformers model from scratch, we can simply use the existing pre-trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIju9lRSTOu-"
      },
      "source": [
        "**Notes**: For language understanding (NLU) tasks, such as sentence classification and sequence tagging, we can simply use an **Encoder-only** model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "oyqfQpPvTOu-"
      },
      "source": [
        "## How to use Transformer models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KeeEYlfTOu_"
      },
      "source": [
        "- Use HuggingFace `transformers` package to load a pre-trained model\n",
        "- Create Dataset & Dataloader for training, validation & testing\n",
        "- Setting hyperparameter including optimizer\n",
        "- Run training for N epochs\n",
        "    - Retrieve a batch of data\n",
        "    - Compute output & loss\n",
        "    - Perform backpropagation\n",
        "    - Update model using the optimizer\n",
        "    - Run validation per epoch, early stopping if needed\n",
        "- Evaluate the trained model on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "pVTzuqZlTOu_"
      },
      "source": [
        "## Load Model\n",
        "https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "n0BT1YHHTOu_",
        "outputId": "42e6bc5b-87fc-4992-bcf0-34960aed2251"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load Tokenizer and Config\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
        "\n",
        "# Instantiate model\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0c6JKGr-TOu_",
        "outputId": "e31116f7-6e1f-4953-e54f-2b9076f87882"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9M66jDVTOvA",
        "outputId": "9150bbd6-cb42-468c-d7a2-bd2123e5cc90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "124443651"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_param(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYcm5qKaTOvA"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQR9unPCTOvA"
      },
      "outputs": [],
      "source": [
        "train_dataset_path = './dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
        "valid_dataset_path = './dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
        "test_dataset_path = './dataset/smsa_doc-sentiment-prosa/test_preprocess.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaVjrbfmTOvA"
      },
      "outputs": [],
      "source": [
        "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
        "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
        "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
        "\n",
        "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)  \n",
        "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)  \n",
        "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH7T3j3XTOvA",
        "outputId": "ccdb4f0c-e0e2-41d3-8dca-b53d3172b4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
            "{0: 'positive', 1: 'neutral', 2: 'negative'}\n"
          ]
        }
      ],
      "source": [
        "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
        "print(w2i)\n",
        "print(i2w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TxmzBHlTOvA"
      },
      "source": [
        "## Fine Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyJHcEhsTOvA"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=5e-6)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0ecO4CMTOvB",
        "outputId": "f470d112-5a02-4bd7-97d7-6a9a12f82971"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 1) TRAIN LOSS:0.2960 LR:0.00000500: 100%|█| 344/344 [01:31<00:00,  3.75it\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 1) TRAIN LOSS:0.2960 ACC:0.89 F1:0.85 REC:0.82 PRE:0.88 LR:0.00000500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.1760 ACC:0.93 F1:0.90 REC:0.90 PRE:0.91: 100%|█| 40/40 [00:06<00:00\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 1) VALID LOSS:0.1760 ACC:0.93 F1:0.90 REC:0.90 PRE:0.91\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 2) TRAIN LOSS:0.1368 LR:0.00000500: 100%|█| 344/344 [01:31<00:00,  3.75it\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 2) TRAIN LOSS:0.1368 ACC:0.95 F1:0.94 REC:0.94 PRE:0.94 LR:0.00000500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.1785 ACC:0.93 F1:0.91 REC:0.90 PRE:0.91: 100%|█| 40/40 [00:06<00:00\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 2) VALID LOSS:0.1785 ACC:0.93 F1:0.91 REC:0.90 PRE:0.91\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.0946 LR:0.00000500: 100%|█| 344/344 [01:31<00:00,  3.74it\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.0946 ACC:0.97 F1:0.96 REC:0.96 PRE:0.97 LR:0.00000500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.1729 ACC:0.94 F1:0.91 REC:0.90 PRE:0.93: 100%|█| 40/40 [00:06<00:00\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 3) VALID LOSS:0.1729 ACC:0.94 F1:0.91 REC:0.90 PRE:0.93\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 4) TRAIN LOSS:0.0634 LR:0.00000500: 100%|█| 344/344 [01:32<00:00,  3.71it\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 4) TRAIN LOSS:0.0634 ACC:0.98 F1:0.98 REC:0.98 PRE:0.98 LR:0.00000500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.1943 ACC:0.94 F1:0.92 REC:0.91 PRE:0.93: 100%|█| 40/40 [00:06<00:00\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 4) VALID LOSS:0.1943 ACC:0.94 F1:0.92 REC:0.91 PRE:0.93\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 5) TRAIN LOSS:0.0437 LR:0.00000500: 100%|█| 344/344 [01:31<00:00,  3.74it\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 5) TRAIN LOSS:0.0437 ACC:0.99 F1:0.99 REC:0.98 PRE:0.99 LR:0.00000500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.2164 ACC:0.94 F1:0.91 REC:0.89 PRE:0.93: 100%|█| 40/40 [00:06<00:00"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 5) VALID LOSS:0.2164 ACC:0.94 F1:0.91 REC:0.89 PRE:0.93\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        " \n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch_data in enumerate(train_pbar):\n",
        "        # Forward model\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        # Update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss = total_train_loss + tr_loss\n",
        "\n",
        "        # Calculate metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "\n",
        "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
        "            total_train_loss/(i+1), get_lr(optimizer)))\n",
        "\n",
        "    # Calculate train metric\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
        "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
        "\n",
        "    # Evaluate on validation\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    \n",
        "    total_loss, total_correct, total_labels = 0, 0, 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(pbar):\n",
        "        batch_seq = batch_data[-1]        \n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "        \n",
        "        # Calculate total loss\n",
        "        valid_loss = loss.item()\n",
        "        total_loss = total_loss + valid_loss\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
        "        \n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
        "        total_loss/(i+1), metrics_to_string(metrics)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03r24yDJTOvB",
        "outputId": "a82b80ae-3ebc-45a6-ce5f-e82a1ff993f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST Metrics | ACC:0.90 F1:0.86 REC:0.84 PRE:0.92\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "    list_label += batch_label\n",
        "metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "print(\"TEST Metrics | {}\".format(metrics_to_string(metrics)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phexq5YfTOvB"
      },
      "source": [
        "## Test fine-tuned model on sample sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcXTtw0CTOvB",
        "outputId": "ccf8af04-d003-490b-e069-d4abaf92dc3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : positive (99.747%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFW9Hm7aTOvB",
        "outputId": "96526ddc-87d8-40c7-de55-fbd9477bd248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Budi pergi ke pondok indah mall membeli cakwe | Label : neutral (99.690%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Budi pergi ke pondok indah mall membeli cakwe'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRNFiIkgTOvC",
        "outputId": "24864b07-2e83-4abf-97be-2c973f0e2424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Dasar anak sialan!! Kurang ajar!! | Label : negative (99.895%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Dasar anak sialan!! Kurang ajar!!'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jHTesobTOvC"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "qei9ZC9oTOvC"
      },
      "source": [
        "- What we have learnt:\n",
        "    - Bag-of-words (BOW)\n",
        "    - TF-IDF\n",
        "    - Word Vector (fasttext)\n",
        "    - A brief introduction to deep learning\n",
        "    - How to use BERT model for Sentiment Analysis\n",
        "<br/> <br/>\n",
        "- What we can conclude:\n",
        "    - Tranditional approach employs sequence statistics (word occurences (TF), domain frequency (DF), etc) to derive features\n",
        "    - Word vector model can capture word level semantic\n",
        "    - Neural network is simply a linear / logistic regression model with multiple layers\n",
        "    - Deep learning can help to process unstructure data with dynamic dimension\n",
        "    - Pre-trained BERT model can achieve much better result compared to traditional approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U0SEWB_TOvC"
      },
      "source": [
        "**notes**: If we tune more, we can achieve an even higher results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsqfeZfYTOvC"
      },
      "source": [
        "<img src=\"https://github.com/BilyHakim/indonlu/blob/master/tutorial/indonlu_result.png?raw=1\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj57NkxDTOvC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env_nusa_exp)",
      "language": "python",
      "name": "env_nusa_exp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}